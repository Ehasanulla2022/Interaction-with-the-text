import os
import requests
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
import openai

# Set OpenAI API key
openai.api_key = "your_openai_api_key"

# 1. Data Ingestion
def scrape_website(url):
    """Crawl and scrape content from a given website URL."""
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        # Extract relevant content (e.g., text from <p>, <h1>-<h3>)
        texts = []
        for tag in soup.find_all(["h1", "h2", "h3", "p"]):
            texts.append(tag.get_text(strip=True))
        return " ".join(texts)
    except requests.exceptions.RequestException as e:
        print(f"Failed to fetch {url}: {e}")
        return ""

def chunk_text(text, chunk_size=500, overlap=50):
    """Split text into smaller chunks for better granularity."""
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks

def embed_and_store_chunks(chunks, embedding_model, db_path="faiss_index"):
    """Generate embeddings for text chunks and store them in a vector database."""
    embeddings = embedding_model.encode(chunks, convert_to_tensor=True)
    faiss_db = FAISS.from_embeddings(chunks, embeddings)
    faiss_db.save_local(db_path)
    return faiss_db

# 2. Query Handling
def retrieve_relevant_chunks(query, faiss_db, embedding_model):
    """Retrieve relevant chunks from the vector database for a given query."""
    query_embedding = embedding_model.encode(query, convert_to_tensor=True)
    docs = faiss_db.similarity_search_by_vector(query_embedding, k=5)
    return docs

# 3. Response Generation
def generate_response(query, docs):
    """Generate a response using an LLM based on retrieved chunks."""
    context = "\n".join([doc.page_content for doc in docs])
    prompt = (
        f"Answer the following query based on the context provided:\n\n"
        f"Query: {query}\n\n"
        f"Context:\n{context}\n\n"
        "Provide a detailed and factual response."
    )
    response = openai.Completion.create(
        engine="text-davinci-003", prompt=prompt, max_tokens=500
    )
    return response.choices[0].text.strip()

# Main Function to Scrape Websites and Query
def main(websites, query, embedding_model_name="sentence-transformers/all-MiniLM-L6-v2"):
    # Initialize embedding model
    embedding_model = SentenceTransformer(embedding_model_name)

    # Scrape websites and preprocess content
    all_chunks = []
    for website in websites:
        print(f"Scraping website: {website}")
        text = scrape_website(website)
        if text:
            chunks = chunk_text(text)
            all_chunks.extend(chunks)

    if not all_chunks:
        print("No content was scraped. Please check the websites.")
        return

    # Embed and store in vector database
    print("Embedding and storing chunks in vector database...")
    faiss_db = embed_and_store_chunks(all_chunks, embedding_model)

    # Retrieve relevant chunks for the query
    print(f"Processing query: {query}")
    docs = retrieve_relevant_chunks(query, faiss_db, embedding_model)
    if not docs:
        print("No relevant data found for the query.")
        return

    # Generate and return a response
    print("Generating response...")
    response = generate_response(query, docs)
    print("\nResponse:\n", response)

if __name__ == "__main__":
    # List of websites to scrape
    websites = [
        "https://www.uchicago.edu/",
        "https://www.washington.edu/",
        "https://www.stanford.edu/",
        "https://und.edu/"
    ]

    # User query
    query = "What are the academic programs offered at the University of Chicago?"

    # Run the main function
    main(websites, query)
